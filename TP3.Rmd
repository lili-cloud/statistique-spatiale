---
title: "TP3 et tp4"
output: html_document
date: "2024-02-08"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


les packages utilisés

```{r}
# Chargement des packages
library(dplyr)
library(sf) 
library(mapsf) 
library(classInt) 
library(leaflet)
library(openxlsx)
```
## Exercice 1

Le but de cet exercice est de discrétiser une variable continue et d’en observer les diﬀérents résultats selon la méthode choisie


### Question1

 Commencez par vous créer votre jeu de données (jointure et création de variable). Attention avant de joindre vos données, il vous faudra d’abord homogénéiser la commune de Paris. Dans un ﬁchier (fond communal), Paris est renseigné sous son code communal (75056). Dans l’autre, Paris est renseigné par arrondissement (75101 à 75120). Vous devrez donc regrouper les arrondissements pour avoir une seule ligne pour Paris. Cette ligne sera renseignée avec le CODGEO 75056

```{r}
# Import des donnees 
# Fond communes France metropolitaine
communes_fm<- st_read("Fonds_carte/France_metro/commune_francemetro_2021.shp", options = "ENCODING=WINDOWS-1252") %>% 
  select(code,libelle,surf)

# Import des population légales des communes en 2019
pop_com_2019<-openxlsx::read.xlsx("Fonds_carte/Donnees/Pop_legales_2019.xlsx")

# Correction pour la ville de Paris
pop_com_2019<-pop_com_2019 %>% 
  mutate(COM=if_else(substr(COM,1,3)=="751","75056",COM)) %>% 
  group_by(code=COM) %>% 
  summarise(pop=sum(PMUN19))

# Jointure
communes_fm<-communes_fm %>% 
  left_join(pop_com_2019,
            by="code") %>% 
  mutate(densite=pop/surf)
```

### Question 2:

Regarder rapidement la distribution de la variable de densite

```{r}
summary(communes_fm$densite)
hist(communes_fm$densite, main = "Distribution de la densité", xlab = "Densité")
```
```{r}
library(ggplot2)

# Créer un histogramme avec ggplot
ggplot(communes_fm, aes(x = densite)) +
  geom_histogram(binwidth = 20, fill = "skyblue", color = "black") +
  labs(title = "Distribution de la densité",
       x = "Densité",
       y = "Fréquence")

```
### Question 3:

On souhaite représenter la variable de densite sous forme d’une carte choroplèthe. Faire cela en utilisant la fonction plot(). La variable de densité sera selectionnée par les crochets sur le modèle suivant : ma_table["ma_variable_continue"]. Ajouter également l’argument border=FALSE pour supprimer les bordures des polygônes.

```{r}
# Sélectionner la variable de densité
densite <- communes_fm["densite"]

# Tracer la carte choroplèthe
plot(densite, border = FALSE)

```
### Question 4:

On se rend compte que la carte est peu informative et qu’il vaut mieux discrétiser notre variable.
Représenter le résultat de la discrétisation de la densité de population selon les méthodes des quantiles, jenks et des écarts-types, ainsi que la méthode pretty. Vous utiliserez la fonction plot en ajoutant l’argument breaks= + le nom de la méthode. Vous analyserez les diﬀérences entre les diﬀérentes cartes.
Laquelle retiendriez-vous? Pourquoi?

```{r}
# Discrétisation de la variable de densité selon les différentes méthodes
plot(densite, breaks="quantile", main="quantile", border = FALSE)
plot(densite, breaks="sd", main="sd", border = FALSE)
plot(densite, breaks="jenks", main="jenks", border = FALSE)
plot(densite, breaks="pretty", main="pretty", border = FALSE)
```
Les cartes sont différentes suivant la méthode utilisée. En utilisant la méthode des quantiles(dans chaque classe il met le même nombre de communes, il y'a un étalement , elle n'est pas trop bonne) , sd(-pour des distributions qui ressemblent à celle normale, construit des classes autour de la moyenne; sd ne marche pas ici car notre distribution n'est pas normale); jenks(comme kmeans, construit des classe qui sont les plus différentes entre elles et plus homogènes dans chaque classe , on aura donc les plus grosses valeurs entre elles)....pretty(essaie de faire des classes jolies)
ici on laisse faire les breaks automatiquement


### Question 5:

Pour obtenir une classiﬁcation satisfaisante, il faut pouvoir comparer la distribution de la variable continue avec celle de la variable discrétisée. Le package classInt est très utile pour construire et analyser les classes.

a. Discrétiser la variable de densité avec la fonction classInt::classIntervals avec la méthode des quantiles (argument style) et 5 classes (argument n). Vous pourrez vous appuyer sur le modèle suivant :

l'objet obtenu contient deux informations: brks (les bornes des intervalles) et var (les valeurs). Plus précisément: 

```{r}
denspop_quant <- classIntervals(
  communes_fm$densite,
  style = "quantile", 
  n = 5
)
str(denspop_quant)
head(denspop_quant$var)
denspop_quant$brks 
```


b. Construire une palette de couleurs avec le code suivant:

```{r}
pal1 <- RColorBrewer::brewer.pal(n = 5, name = "YlOrRd")
```

Représenter ensuite l’objet précédent (découpage quantile) avec la fonction plot etcette palette de couleur (argument pal=). Vous ajouterez l’argument main à votre fonction plot pour préciser un titre. Analyser le graphique.

```{r}
plot(
  denspop_quant,
  pal = pal1,
  main = "quantile"
)
```
on a la fonction de répartition de notre var de départ, les communes sont assez pétites,  la derniere classe est très étendue, ce qui est un peu genant.

c. Relancer l’analyse pour les méthodes sd, jenks et pretty. 

```{r}
analyser_discret <- function(method, nb_classes){
  denspop_c <- classIntervals(
    communes_fm$densite,
    style = method, 
    n = nb_classes
  )
  print(denspop_c$brks)
  plot(
    denspop_c,
    pal = pal1,
    main = method
  )
  return(denspop_c)
}
# Avec cinq classes:
all_discret <- sapply(c("quantile", "sd","pretty","jenks"), analyser_discret, nb_classes = 5)
```


```{r}
# A partir des informations obtenues, on peut définir nos propres intervalles. 
quantile(communes_fm$densite, probs = seq(0,1,0.1))
summary(communes_fm$densite)
#40 = médiane
#162 = moyenne
#on reprend certaines bornes de Jenks - en fusionnant les derniers intervalles
# Un exemple de découpage manuel avec 7 classes
denspop_man_brks7 <- c(0,40,162,500,1000,4000,8000,27200)
# Un exemple de découpage manuel avec 5 classes
denspop_man_brks5 <- c(0,40,162,1000,8000,27200)
```
d. Finalement, on décide de discrétiser notre variable avec les bornes suivantes : [0;40[,[40;162[,[162;1000[,[1000;8000[ et [8000;27200[. Ajouter la variable discrétisée dans le fond communal. Vous utiliserez la fonction `cut`. Vous ferez attention à l'inclusion des bornes inférieures et à l'exclusion des bornes supérieures.

```{r}
popcomfm_sf <- communes_fm %>%
  mutate(
    densite_c = cut(
      densite,
      breaks = denspop_man_brks5,
      include.lowest = TRUE,
      right = FALSE,
      ordered_result = TRUE
    )
  )
```

e. Analyser la distribution de cette variable. Représenter la variable discrétisée sur une carte, en créant préalablement une nouvelle palette de couleurs ayant le bon nombre de classes.

```{r}
table(popcomfm_sf$densite_c)
pal2 <- c(
  RColorBrewer::brewer.pal(
  n=5,
  name="Greens"
  )[4:3],
  RColorBrewer::brewer.pal(
  n=5,
  name="YlOrRd"
  )[c(2,4:5)]
)
plot(
  popcomfm_sf["densite_c"], 
  pal=pal2, 
  border = FALSE,
  main = "Densité de population",
  )
```


## Exercice 2

Représenter sous forme de carte le taux de pauvreté par département. Vous utiliserez le package mapsf. Vous trouverez de la documentation sur ce package ici. Vous utiliserez le fond “dep_francemetro_2021” ainsi que le ﬁchier “Taux_pauvrete_dept_2021.xlsx” présent dans le dossier “U:/Eleves/Cartographie/Donnees”. Pour l’import de ce ﬁchier, vous pouvez utiliser la fonction openxlsx::read.xlsx().

### Question1

Dans un premier temps, vous pourrez essayer de faire 3 cartes basiques : une en découpant la variables d’intérêt selon la méthode de Fisher (breaks=“ﬁsher”), une autre avec des classes de même amplitude (“equal”) et enﬁn selon la méthode des quantiles (“quantile”)

```{r}
library(mapsf)
library(openxlsx)
library(sf)

```

```{r}
# Lire le fichier Excel
donnees <- openxlsx::read.xlsx("Fonds_carte/Donnees/Taux_pauvrete_2018.xlsx")


```

```{r}
# Charger le fond de carte
dep= st_read("Fonds_carte/France_metro/dep_francemetro_2021.shp", options = "ENCODING=WINDOWS-1252") 

dep= dep %>%
  left_join(donnees%>%select(-Dept),
            by=c("code"="Code"))
```


Méthode de Fisher
```{r}
mf_map(x= dep,
       var = "Tx_pauvrete",
       type = "choro",
       nbreaks = 4,
       breaks = "jenks"
)

```

Méthode des classes de même amplitude

```{r}
mf_map(x= dep,
       var = "Tx_pauvrete",
       type = "choro",
       nbreaks = 4,
       breaks = "equal"
)
```



```{r}
mf_map(x= dep,
       var = "Tx_pauvrete",
       type = "choro",
       nbreaks = 4,
       breaks = "quantile"
)
```

### Question2
Dans un 2e temps, vous ferez un découpages manuel avec les seuils suivants : 0, 13, 17, 25, max(Tx_pauvrete). La carte contiendra également un zoom sur Paris et sa petite couronne (departements 75, 92, 93, 94).

creation d'un encadre pour notre carte
```{r}
mf_map(x= dep,
       var = "Tx_pauvrete",
       type = "choro",
       nbreaks = 4,
       breaks = "quantile"
)
mf_inset_on(x= dep, pos = "topright",
            cex= ,2)
mf_init(dep %>%
          filter(code  %in% c("75", "92", "93", "94")))
mf_map(dep %>%
         filter(code  %in% c("75", "92", "93", "94")),
       var = "Tx_pauvrete",
       type = "choro",
       breaks = c(0, 13, 17, 25, max(dep$Tx_pauvrete)),
       pal= couleur,
       leg_pos = NA)

#mf_inset_on(x= dep,pos= "topright",
            )


```
```{r}

```




# TP 4: Introduction à la statistique spatiale

## **OBJECTIFS DU TP:**

- Le but de ce TP est de s'iniitier à l'étude de l'autocorrélation spatiale sur données surfaciques.  


Aﬁn d’utiliser une version de R plus récente (et une version du package sf plus récente aussi), vous travaillerez sur le datalab (plateforme du sspcloud, service de l’Insee) : https://datalab.sspcloud.fr. 

Les fonds et données nécessaires au TP sont disponibles sous “U:/Eleves/Cartographie/TP4.zip".

En guise de référence, le mieux est de se reporter au [manuel d'analyse spatiale de l'Insee](https://www.insee.fr/fr/information/3635442)

You can also have a look to [this link](https://mgimond.github.io/Spatial/spatial-autocorrelation-in-r.html#app8_2)




---

Commencer par créer un projet pour cette séance de TP. Vous placerez votre projet dans un répertoire personnel. Ouvrir un nouveau programme R. Vous aurez besoin des packages suivants pour le TP :

```{r}
# Chargement des packages
library(dplyr)
library(sf)
library(spdep)
library(RColorBrewer)

```



### Exercice 1

Nous allons étudier s'il existe un phénomène d'autocorrélation spatiale des revenus médians par iris marseillais.

Vous utiliserez le fond des communes des Iris France entiere ainsi que les données de revenus.

1. Commencez par vous créer votre jeu de données : Sélectionnez la ville de Marseille sur votre fond d'Iris et faire la jointure avec votre jeu de données.

```{r}
# Import des données
iris<-st_read("./Fonds_carte/iris_franceentiere_2021.shp")
data<-read.csv2("./Fonds_carte/BASE_TD_FILO_DISP_IRIS_2018.csv",sep=";",dec=".")

# Jointure
marseille<-iris %>% 
  filter(substr(depcom,1,3)=="132") %>% 
  left_join(
    data %>% select(code=IRIS,DISP_MED18),
    by="code"
  )
```

2. Le système de projection de votre table est en WGS84, convertissez-le en Lambert-93 (EPSG 2154)


```{r}
marseille= st_transform(marseille, 2154)
```




3. Faites un premier résumé statistique de la variable de revenu médian. Faites également un boxplot du revenu moyen en fonction des arrondissements.

```{r}
# Résumé statistique du revenu médian
summary(marseille$DISP_MED18)
summary(data %>% select(DISP_MED18))

ggplot(marseille, aes(x= depcom, y= DISP_MED18)) +
  geom_boxplot()

# Boxplot du revenu moyen en fonction des arrondissements
boxplot(marseille$DISP_MED18 ~ marseille$depcom, 
        main = "Revenu médian par arrondissement",
        xlab = "Arrondissement",
        ylab = "Revenu médian")



```



4. Supprimer les valeurs manquantes puis représenter la carte de Marseille en fonction des revenus. Vous pouvez utiliser la fonction plot (n'hésitez pas à utiliser une ou plusieurs méthodes de discrétisation automatique - argument `breaks`). Au vu de la carte, vous semble-t-il y avoir un phénomène spatial dans la distribution des revenus ?

```{r}
# Supprimer les valeurs manquantes
marseille <- na.omit(marseille)

# Tracer la carte de Marseille en fonction des revenus
plot(marseille["DISP_MED18"], main= "Revenus à Marseille")

```

les zones bleues sont dans le centre et le nord de marseille, on a une concentraition d'irus revenu à faible revenu médian
a priori la distribution spatiale au sein de marseille est plutot corrélée positivement: les personnes à revunu élevé vivent ensemble.
```{r}
#Discrétisation automatique des revenus 

plot(marseille["DISP_MED18"], main= "Revenus à Marseille", breaks = "quantile")
plot(marseille["DISP_MED18"], main= "Revenus à Marseille", breaks = "jenks")
```




5. Pour nous faire une première idée de la dimension spatiale de la distribution des revenus,
nous allons représenter les mêmes revenus mais distribués de manière aléatoire au sein
des iris marseillais. On pourra ainsi comparer la carte de la distribution réelle des revenus avec la carte de la distribution aléatoire. Pour cela,

a. Créez une permutation aléatoire des revenus disponibles médians par iris avec la fonction `sample()` et à partir de la variable `DISP_MED18` du fond des iris de Marseille. 
Vous stockerez ce vecteur dans une nouvelle variable du fond des iris nommée `DISP_MED18_ALEA`.

```{r}
# Importer les bibliothèques nécessaires
library(sf)


# Créer une permutation aléatoire des revenus médians par iris
iris$DISP_MED18_ALEA <- sample(iris$DISP_MED18) 
                                         
```




```{r}
# Visualiser la distribution géographique de la variable aléatoire
plot(iris["DISP_MED18_ALEA"], main = "Distribution aléatoire des revenus médians par iris")

# Comparer avec la distribution réelle
plot(iris["DISP_MED18"], main = "Distribution réelle des revenus médians par iris")
```



```{r}

```


b. représentez sur une carte la distribution géographique de la variable que vous venez de créer. 
Comparez le résultat avec la carte réalisée sur la distribution réelle. La distribution spatiale réelle des revenus est-elle proche de la distribution aléatoire ?


la distribution spatiale du revenu median au sein de marseille ne semble pas être aléatoire

6. Pour corroborer la conclusion du 5., nous allons mesurer et tester le phénomène d'autocorrélation spatiale.

>Un phénomène est autocorrélé spatialement quand la valeur de la variable étudiée à un 
endroit donné est plus liée aux valeurs de ses voisins plutôt qu'à celles des autres. 
On parle d'*autocorrélation positive* si des voisins ont tendance à prendre des valeurs similaires et
d'*autocorrélation négative* si des voisins ont tendance à prendre des valeurs différentes.

a. Quel type d'autocorrélation spatiale, le phénomène étudié semble-t-il avoir ?

Nous avons une corrélation positive

b. Pour étudier le phénomène, il nous faut construire une matrice de voisinage. 
Il existe plusieurs façons de définir le voisinage de nos iris. Dans un premier temps,
nous allons définir le voisinage par la contiguïté : deux iris sont voisins s'ils sont contigus.

Pour limiter la taille des objets créés, nous allons travailler avec des listes plutôt 
qu'avec des matrices carrées.

Extraire la liste des voisins de chaque Iris. Pour cela, vous utiliserez la fonction 
`spdep::poly2nb()`. Par défaut, il s'agit de la contiguité  dite `QUEEN` qui reprend 
les mouvements de la Reine aux échecs. Prenez connaissance de l'objet créé et réalisez 
un résumé de l'objet en sortie avec la fonction `summary()`.


c. Combien de voisins a le quatrième élément de la liste ?


7. Nous allons transformer la matrice de contiguité en une matrice de pondérations. 
L'idée est d'affecter un poids identique à chacun des voisins d'un iris. 

a. Créez une liste de poids à partir de la liste de voisins précédemment créée. 
Pour cela, utilisez la fonction `spdep::nb2listw()`, avec l'argument `zero.policy=TRUE` pour intégrer les Iris n'ayant potentiellement pas de voisins (par défaut, la fonction exclut les observations sans voisin). 



b. Prenez connaissance de l'objet créé avec la fonction `str()` et l'argument `max.level = 1` et réalisant un résumé de la liste avec la fonction `summary()`.



c. Vérifiez que la somme des pondérations associées à chaque pondération est égale à 1.



8. Une autre façon très visuelle de vérifier la présence d'une autocorrélation est de dresser le diagramme de Moran. La matrice de pondération calculée en 7. va nous permettre de le calculer.

a. Créer une variable des revenus disponibles centrés réduits avec la fonction `scale()`. 
Vous la nommerez `DISP_MED18_STD` et l'ajouterez au fond des iris de Marseille.
Vous vérifierez que la nouvelle variable est bien centrée (moyenne = 0) et réduite (SD = 1).



b. Dresser le diagramme de Moran avec la fonction `moran.plot()` à partir de la variable standardisée (utiliser la fonction `as.numeric()` si un problème apparaît).
Le second argument à préciser (`listw`) correspond à la liste des poids des voisins que vous avez créée précédemment.


c. Le diagramme de Moran représente, pour chaque observation (ici un Iris), croise deux informations :  
- en abscisse, est représenté le revenu médian disponible observé au sein de l'iris (variable centrée réduite);
- en ordonnées, est représentée la moyenne des revenus médians des voisins de l'iris observé.

Interprétez les quatre cadrans du diagramme.

nuage de points assez allogé donc autocorrélation positive*revenus bas entouré de revenus élevés
revenus elevé entouré de revenus bas
revenus bas entouré de revenus bas
revenus hauts entouré de revenus hauts

d. D'après le diagramme de Moran, les revenus médians semblent-ils autocorrélés spatialement ? Si oui, l'autocorrélation vous semble-t-elle positive ou négative ?
nuage de points assez allogé donc autocorrélation positive*
 
9. Il existe une mesure globale de l'autocorrélation spatiale d'un phénomène. Il s'agit du 
**I de Moran**. 
il varie entre -1 et 1

a. Calculez cet indice et sa significativité avec la fonction `spdep::moran.test()` 
utilisée de la façon suivante : `moran.test(marseille$DISP_MED18_STD, ponderation, randomisation = TRUE)`.
Le dernier argument signifie que la distribution observée est comparée à une distribution 
aléatoire obtenue par permutation des valeurs observées. 
0,701 stat de Moran
pvalue= 10 puissance -7, faible donc supérieure au seuil 5? ON REJETTE h0 donc ceci confirme l'autocorrélation positive
c'est un indicateur global donc il s'interpréte au sens de la commune, c'est à dire la distribution de revenus à marseille n'est pas aléatoire. 

b. Interpértez le résultat obtenu : confirme-t-il ou non votre hypothèse ?



10. BONUS - DECOUVRIR LES INDICATEURS D'AUTOCORRELATION LOCAUX.

L'indice de Moran est un indicateur global de mesure de l'autocorrélation. Mais, 
ce phénomène peut connaître une intensité très différente localement. Dans certains endroits de 
la ville de Marseille, la ressemblance des voisins peut être très forte et à d'autres 
endroits plus lâche. Des indicateurs locaux de mesure de l'autocorrélation spatiale sont nécessaires 
pour compléter l'analyse de la distribution spatiale des revenus disponibles médians à Marseille.
Nous calculerons pour cela les ***LISA*** (*Local Indicators of Spatial Association*), 
ou ***I de Moran locaux***.

a. Calculez les Lisa avec la fonction `spdep::localmoran()` et stockez le résultat 
dans un objet appelé `mars_rev_lisa`.


b. Etudiez l'objet obtenu, en utilisant notamment les fonctions `class()`, `str(.,max.level=1)` et 
`summary()`.


c. Quelle est la moyenne des indicateurs locaux ($I_i$)?



d. L'interprétation d'un Lisa est tout à fait similaire à l'indice global. 
Si l'indicateur local d'un Iris donné est positif, cela signifie qu'il est entouré d'Iris ayant des niveaux de revenus similaires. 
S'il est négatif, cela indique qu'il est plutôt entouré d'Iris ayant des niveaux de revenus différents (opposés).

Combien d'indicateurs locaux sont-ils négatifs ?



e. Nous cherchons à représenter les Lisa sur une carte des Iris marseillais. Pour cela, ajouter les Lisa comme une nouvelle variable du fond des iris, variable que vous nommerez `LISA`.


f. Interprétez ce que vous voyez sur la carte.



g. Comme pour le I de Moran, il est nécessaire avant d'aller plus loin dans l'interprétation, de savoir si les Lisa calculés sont significativement différents de zéro. Dans l'objet `mars_rev_lisa`, repérez la colonne correspondant à la pvaleur du test associé à la mesure des Lisa et placez-la dans une nouvelle variable du fond des Iris intitulée `LISA_PVAL`.




h. Combien de LISA sont-ils significativement différents de zéro pour un niveau de confiance à 95% ?



i. Représentez sur une carte la p-valeur des LISA en choisissant les bornes d'intervalles suivantes : 0,0.01,0.05,0.1,1.



j. Les zones précédemment repérées sur la carte des LISA font-elles parties des zones 
où les LISA sont les plus significatifs ?






